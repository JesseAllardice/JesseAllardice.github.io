---
title: "Senior Machine Learning Researcher, Multimodal Foundation Models"
company:
  name: "Apple"
  logo: "/img/companies/apple.png"
employment_type: "Full-time"
start_date: 2022-07-01
end_date: 
location: "Cupertino, California, United States · Hybrid"
duration: "4 yrs 3 mos"
skills: ["Multimodal Learning", "Large Language Models", "Computer Vision", "Natural Language Processing", "Machine Learning", "Research", "Python", "PyTorch"]
excerpt: "Researching Multimodal Large Language Models (MLLMs) models to solve novel tasks in image/video understanding and generation."
---

Researching Multimodal Large Language Models (MLLMs) models to solve novel tasks in image/video understanding and generation.

**Key Research Areas:**
- Multimodal foundation models for agentic applications
- Image and video understanding with language models
- Flexible-length image tokenization (FlexTok)
- Benchmark-targeted ranking for efficient model training (BETR)

**Major Contributions:**
- Led development of FlexTok methodology, presented at ICML 2025
- Achieved 1.8–2.8× compute multipliers with BETR approach
- Advanced post-training capabilities of multimodal foundation models

**Publications:**
- FlexTok: Resampling Images into 1D Token Sequences of Flexible Length (ICML 2025)
- BETR: Benchmark-Targeted Ranking for Language Model Pretraining (2025)

This role builds on my experience from the Apple AI Residency Program and focuses on pushing the boundaries of multimodal AI systems that can understand and interact with complex visual and textual environments.