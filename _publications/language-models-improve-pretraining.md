---
title: "Language Models Improve When Pretraining Data Matches Target Tasks"
authors: ["David Mizrahi", "Anders Boesen Lindbo Larsen", "**Jesse Allardice**", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"]
venue: "arXiv preprint"
year: 2025
paper_url: "https://arxiv.org/abs/2507.12466"
image: "/img/publications/betr-scaling-laws.png"
tags: ["Language Models", "Pretraining", "Data Filtering"]
links:
  - type: "pre-print"
    url: "https://arxiv.org/abs/2507.12466"
---

We investigate how the alignment between pretraining data and target tasks affects language model performance, showing that models benefit significantly when pretraining data closely matches the distribution of downstream tasks.