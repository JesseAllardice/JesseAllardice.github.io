---
title: "BETR: Benchmark-Targeted Ranking for Language Model Pretraining"
authors: ["Jesse Allardice", "et al."]
year: 2025
type: "preprint"
organization: "Apple"
image: "/img/publications/betr.jpg"
links:
  - type: "paper"
    url: "#"
tags: ["language models", "pretraining", "efficiency"]
excerpt: "A new method for matching pretraining data to benchmarks in language models, achieving 1.8–2.8× compute multipliers over strong baselines."
---

A new method for matching pretraining data to benchmarks in language models, demonstrating significant efficiency improvements in compute utilization.

**Key Results:**
- **1.8–2.8× compute multipliers** over strong baselines
- Novel data selection methodology for language model pretraining
- Significant breakthrough in language model efficiency

**Impact:**
This research demonstrates how strategic data selection can dramatically improve language model performance while reducing computational requirements. The work has important implications for making large language models more accessible and environmentally sustainable.